numericVarNames <- names(numericVars) #saving names vector for use later
train_numVar <- train[, numericVars]
model.1 = lm(SalePrice ~ ., data = na.omit(train_numVar))
summary(model.1)
vif(model.1)
alias(model.1)
model.2 = lm(SalePrice ~ OverallQual + MSSubClass + LotArea + LotFrontage + OverallCond +
YearBuilt + MasVnrArea + BsmtFinSF1 + X1stFlrSF + X2ndFlrSF + BsmtFullBath +
BedroomAbvGr + KitchenAbvGr + TotRmsAbvGrd + Fireplaces +  GarageCars +
WoodDeckSF + ScreenPorch + PoolArea, data = na.omit(train_numVar))
vif(model.2)
model.3 = lm(SalePrice ~ OverallQual + MSSubClass + LotArea + LotFrontage + OverallCond +
YearBuilt + MasVnrArea + BsmtFinSF1 + BsmtFullBath +
BedroomAbvGr + KitchenAbvGr + Fireplaces +  GarageCars +
WoodDeckSF + ScreenPorch + PoolArea, data = na.omit(train_numVar))
vif(model.3)
model.4 = lm(SalePrice ~ OverallQual + MSSubClass + LotArea + LotFrontage + OverallCond +
YearBuilt + MasVnrArea + BsmtFinSF1 + BsmtFullBath +
BedroomAbvGr + KitchenAbvGr + Fireplaces +  GarageCars +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + GrLivArea, data = na.omit(train_numVar))
vif(model.4)
AIC(model.1)
AIC(model.2)
AIC(model.3)
AIC(model.4)
#marius' model
model.5 = lm(SalePrice ~ GrLivArea + GarageArea + TotalBsmtSF + X1stFlrSF + MasVnrArea +
BsmtFinSF1 + WoodDeckSF + LotFrontage  X2ndFlrSF    OpenPorchSF  LotArea
BsmtUnfSF + PoolArea  + X3SsnPorch + LowQualFinSF + BsmtFinSF2, data = na.omit(train_numVar))
#marius' model
model.5 = lm(SalePrice ~ GrLivArea + GarageArea + TotalBsmtSF + X1stFlrSF + MasVnrArea +
BsmtFinSF1 + WoodDeckSF + LotFrontage  X2ndFlrSF + OpenPorchSF + LotArea +
BsmtUnfSF + PoolArea  + X3SsnPorch + LowQualFinSF + BsmtFinSF2, data = na.omit(train_numVar))
#marius' model
model.5 = lm(SalePrice ~ GrLivArea + GarageArea + TotalBsmtSF + X1stFlrSF + MasVnrArea +
BsmtFinSF1 + WoodDeckSF + LotFrontage  X2ndFlrSF + OpenPorchSF + LotArea +
BsmtUnfSF + PoolArea  + X3SsnPorch + LowQualFinSF + BsmtFinSF2, data = na.omit(train_numVar))
#marius' model
model.5 = lm(SalePrice ~ GrLivArea + GarageArea + TotalBsmtSF + X1stFlrSF + MasVnrArea +
BsmtFinSF1 + WoodDeckSF + LotFrontage  X2ndFlrSF + OpenPorchSF + LotArea +
BsmtUnfSF + PoolArea  + X3SsnPorch + LowQualFinSF + BsmtFinSF2, data = na.omit(train_numVar))
#marius' model
model.5 = lm(SalePrice ~ GrLivArea + GarageArea + TotalBsmtSF + X1stFlrSF + MasVnrArea +
BsmtFinSF1 + WoodDeckSF + LotFrontage + X2ndFlrSF + OpenPorchSF + LotArea +
BsmtUnfSF + PoolArea  + X3SsnPorch + LowQualFinSF + BsmtFinSF2, data = na.omit(train_numVar))
vif(model.5)
AIC(model.5)
#Stepwise regression using AIC as the criteria (the penalty k = 2).
forwardAIC = step(model.empty, scope, direction = "forward", k = 2)
library(MASS)
library(car)
train = read.csv('train.csv')
library(dplyr)
train = filter(train,GrLivArea <4000)
numericVars <- which(sapply(train, is.numeric)) #index vector numeric variables
numericVarNames <- names(numericVars) #saving names vector for use later
train_numVar <- train[, numericVars]
numericVarNames
model.empty = lm(SalePrice ~ 1, data = na.omit(train_numVar)) #The model with an intercept ONLY.
model.full = lm(SalePrice ~ ., data = na.omit(train_numVar)) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
dim(train_numVar)
#Stepwise regression using AIC as the criteria (the penalty k = 2).
forwardAIC = step(model.empty, scope, direction = "forward", k = 2)
model.6 = lm(SalePrice ~ OverallQual + GrLivArea + BsmtFinSF1 + TotalBsmtSF +
YearRemodAdd + LotArea + GarageArea + KitchenAbvGr + MasVnrArea +
BedroomAbvGr + TotRmsAbvGrd + MSSubClass + YearBuilt + OverallCond +
GarageCars + OpenPorchSF + ScreenPorch + BsmtHalfBath, data = na.omit(train_numVar))
vif(model.6)
AIC(model.6)
model.7 = lm(SalePrice ~ OverallQual + GrLivArea + BsmtFinSF1 + TotalBsmtSF +
YearRemodAdd + LotArea + GarageArea + KitchenAbvGr + MasVnrArea +
BedroomAbvGr + TotRmsAbvGrd + MSSubClass + YearBuilt + OverallCond +
OpenPorchSF + ScreenPorch + BsmtHalfBath, data = na.omit(train_numVar))
vif(model.7)
model.6 = lm(SalePrice ~ OverallQual + GrLivArea + BsmtFinSF1 + TotalBsmtSF +
YearRemodAdd + LotArea + GarageArea + KitchenAbvGr + MasVnrArea +
BedroomAbvGr +  MSSubClass + YearBuilt + OverallCond + OpenPorchSF +
ScreenPorch + BsmtHalfBath, data = na.omit(train_numVar))
model.7 = lm(SalePrice ~ OverallQual + GrLivArea + BsmtFinSF1 + TotalBsmtSF +
YearRemodAdd + LotArea + GarageArea + KitchenAbvGr + MasVnrArea +
BedroomAbvGr +  MSSubClass + YearBuilt + OverallCond + OpenPorchSF +
ScreenPorch + BsmtHalfBath, data = na.omit(train_numVar))
vif(model.7)
AIC(model.7)
summary(model.7)
plot(model.7)
vif(model.7)
avPlots(model.7)
#Multiple R-squared:  0.871,
#Adjusted R-squared:  0.8691
#F-statistic: 464.1 on 16 and 1100 DF,  p-value: < 2.2e-16
plot(model.7)
influencePlot(model.7)
avPlots(model.7)
confint(model.7)
#Creating training and testing sets with 70-30 split
set.seed(0)
train_train_idx = sample(1:nrow(train), 7*nrow(train)/10)
train_train = train[train_train_idx,]
train_test  = train[-train_train_idx,]
x = model.matrix(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_train)
View(x)
x = model.matrix(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_train)[,-1]
View(x)
dim(x)
dim(y)
y = train_train$SalePrice
y.test = train_test$SalePrice
x.test = model.matrix(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_test)[,-1]
dim(y)
len(y)
length(y)
#Running 5-fold cross validation.
set.seed(0)
cv.ridge = cv.glmnet(x, y, lambda = grid, alpha = 0, nfolds = 10)
library(glmnet)
cv.ridge = cv.glmnet(x, y, lambda = grid, alpha = 0, nfolds = 10)
grid = 10^seq(1, -5, length = 100)
cv.ridge = cv.glmnet(x, y, lambda = grid, alpha = 0, nfolds = 10)
train = read.csv('./data/train_superclean.csv')
#train = train %>% mutate(log.SalePrice = log(SalePrice))
test = read.csv('./data/test_superclean.csv')
#Creating training and testing sets with 70-30 split
set.seed(0)
train_train_idx = sample(1:nrow(train), 7*nrow(train)/10)
train_train = train[train_train_idx,]
train_test  = train[-train_train_idx,]
x = model.matrix(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_train)[,-1]
y = train_train$SalePrice
y.test = train_test$SalePrice
x.test = model.matrix(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_test)[,-1]
grid = 10^seq(1, -5, length = 100)
#Running 5-fold cross validation.
set.seed(0)
cv.ridge = cv.glmnet(x, y, lambda = grid, alpha = 0, nfolds = 10)
View(train_train)
dim(train_train)
train_train[, 1:80]
cv.ridge = cv.glmnet(x = train_train[, 1:80], y, lambda = grid, alpha = 0, nfolds = 10)
# best model
model.n5 = lm(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train)
?predict
# best model
model.n5 = lm(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train)
train = read.csv('./data/train_superclean.csv')
#train = train %>% mutate(log.SalePrice = log(SalePrice))
test = read.csv('./data/test_superclean.csv')
# best model
model.n5 = lm(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train)
train = read.csv('./data/train_superclean.csv')
setwd("~/makbi")
train = read.csv('./data/train_superclean.csv')
#train = train %>% mutate(log.SalePrice = log(SalePrice))
test = read.csv('./data/test_superclean.csv')
# best model
model.n5 = lm(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train)
summary(model.n5) #Adjusted R-squared:  0.91  , F-statistic: 135.1
View(train)
View(test)
View(train)
View(train)
test = test[, 1:81]
train = train[, 1:81]
predict(object = model.n5, newdata = test)
predict(object = model.n5, newdata = test)
y.predict = predict(object = model.n5, newdata = test)
y.predict = predict(object = model.n5, newdata = test)
y.predict
class(y.predict)
y.predict = as.data.frame(y.predict)
View(y.predict)
View(y.predict)
y.predict = predict(model.n5, newdata = test)
test_y = cbind(test , y.predict)
View(test_y)
test_y = select(test_y, "Id", "SalePrice")
library(dplyr)
test_y = select(test_y, "Id", "SalePrice")
View(test_y)
SalePrice = predict(model.n5, newdata = test)
test_y = cbind(test , SalePrice)
test_y = select(test_y, "Id", "SalePrice")
View(test_y)
write.csv(test_y, file = 'linear_model_kaggle.csv')
a = read.csv('linear_model_kaggle.csv')
View(a)
write.csv(test_y, file = 'linear_model_kaggle.csv',row.names=FALSE )
a
a  = a[,-1]
p = read.csv('linear_model_kaggle.csv')
p
View(p)
View(a)
is.null(test)
test.isnull()
#train = train %>% mutate(log.SalePrice = log(SalePrice))
test = read.csv('./data/test_superclean.csv')
SalePrice = predict(model.n5, newdata = test)
test_y = cbind(test , SalePrice)
View(test_y)
test_y = select(test_y, "Id", "SalePrice")
View(test_y)
#train = train %>% mutate(log.SalePrice = log(SalePrice))
test = read.csv('cleaned_test_data.csv')
SalePrice = predict(model.n5, newdata = test)
View(test)
#train = train %>% mutate(log.SalePrice = log(SalePrice))
test = read.csv('cleaned_test_data.csv')
View(test)
test = test[, -1]
View(test)
View(train)
View(test)
View(train)
train['OverallQual']
train = read.csv('./data/train_superclean.csv')
View(train)
train = read.csv('AkshayStyleCleanData.csv')
# best model
model.n5 = lm(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train)
train = read.csv('AkshayStyleCleanData.csv')
# best model
model.n5 = lm(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train)
#train = train %>% mutate(log.SalePrice = log(SalePrice))
test = read.csv('cleaned_test_data.csv')
SalePrice = predict(model.n5, newdata = test)
test_y = cbind(test , SalePrice)
train = read.csv('AkshayStyleCleanData.csv')
#train = train %>% mutate(log.SalePrice = log(SalePrice))
test = read.csv('cleaned_test_data.csv')
SalePrice = predict(model.n5, newdata = test)
# best model
model.n5 = lm(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train)
SalePrice = predict(model.n5, newdata = test)
# best model
model.n5 = lm(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train)
summary(model.n5) #Adjusted R-squared:  0.91  , F-statistic: 135.1
vif(model.n5) #
library(MASS)
vif(model.nn5) #
library(car)
library(psych)
vif(model.n5) #
AIC(model.n5) # 33941.43
plot(model.n5)
anova(model.n5)
influencePlot(model.n5)
#MSE
mean(abs(predict(model.n5) - train$log.SalePrice)) #0.08010317
#MSE
mean(abs(predict(model.n5) - train$log(SalePrice))) #0.08010317
#MSE
mean(abs(predict(model.n5) - train$log(SalePrice))) #0.08010317
# best model
model.n5 = lm(log(SalePrice) ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train)
#MSE
mean(abs(predict(model.n5) - train$log(SalePrice))) #0.08010317
train$log(SalePrice)
#MSE
mean(abs(predict(model.n5) - train$(log(SalePrice)))) #0.08010317
#MSE
mean(abs(predict(model.n5) - train$(log(SalePrice)))) #0.08010317
train = read.csv('./data/train_superclean')
setwd("~/makbi")
train = read.csv('./data/train_superclean')
train = read.csv('./data/train_superclean.csv')
sqrt(0.0801)
#train = train %>% mutate(log.SalePrice = log(SalePrice))
test = read.csv('./data/test_superclean.csv')
which(is.na(test))
lasso_model = glmnet(x, y, alpha = 1, lambda = grid)
train = read.csv('./data/train_superclean.csv')
train = train %>% mutate(log.SalePrice = log(SalePrice))
test = read.csv('./data/test_superclean.csv')
# best model
model.n5 = lm(log.SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train)
summary(model.n5) #Adjusted R-squared:  0.91  , F-statistic: 135.1
vif(model.n5) #
AIC(model.n5) # 33941.43
x = model.matrix(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_train)[,-1]
#Creating training and testing sets with 70-30 split
set.seed(0)
train_train_idx = sample(1:nrow(train), 7*nrow(train)/10)
train_train = train[train_train_idx,]
train_test  = train[-train_train_idx,]
x = model.matrix(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_train)[,-1]
y = train_train$SalePrice
y.test = train_test$SalePrice
x.test = model.matrix(SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_test)[,-1]
grid = 10^seq(1, -5, length = 100)
#Running 5-fold cross validation.
set.seed(0)
cv.ridge = cv.glmnet(x = train_train[, 1:80], y, lambda = grid, alpha = 0, nfolds = 10)
plot(cv.ridge, main = "Ridge Regression\n")
cv.ridge = cv.glmnet(x, y, lambda = grid, alpha = 0, nfolds = 10)
plot(cv.ridge, main = "Ridge Regression\n")
x = model.matrix(log.SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_train)[,-1]
y = train_train$SalePrice
y.test = train_test$SalePrice
x.test = model.matrix(log.SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_test)[,-1]
grid = 10^seq(1, -5, length = 100)
#Running 5-fold cross validation.
set.seed(0)
cv.ridge = cv.glmnet(x, y, lambda = grid, alpha = 0, nfolds = 10)
plot(cv.ridge, main = "Ridge Regression\n")
best.lambda.ridge = cv.ridge$lambda.min
best.lambda.ridge # 0.01
plot(cv.ridge, main = "Ridge Regression\n")
#What is the test MSE associated with this best value of lambda?
ridge.models.train = glmnet(x, y, alpha = 0, lambda = grid)
ridge.best.lambda.train = predict(ridge.models.train, s = best.lambda.ridge, newx = x.test)
sqrt(mean((ridge.best.lambda.train - y.test)^2)) #0.04010766
mean(abs(ridge.best.lambda.train - y.test)) #0.0968727 , absolute error
x = model.matrix(log.SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_train)
y = train_train$SalePrice
y.test = train_test$SalePrice
x.test = model.matrix(log.SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_test)
grid = 10^seq(1, -5, length = 100)
#Running 5-fold cross validation.
set.seed(0)
cv.ridge = cv.glmnet(x, y, lambda = grid, alpha = 0, nfolds = 10)
x = model.matrix(log.SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_train)
y = train_train$log.SalePrice
y.test = train_test$log.SalePrice
x.test = model.matrix(log.SalePrice ~ OverallQual +  LotArea + LotFrontage + OverallCond +
MasVnrArea + BsmtFinSF1 + BsmtFullBath + X2ndFlrSF + Fireplaces +
KitchenAbvGr + GarageCars + Condition2 + HeatingQC +
WoodDeckSF + ScreenPorch + PoolArea + X1stFlrSF + KitchenQual + Condition1 +
Utilities +  MSZoning  + SaleCondition + Functional +
BsmtExposure + RoofMatl + YearRemodAdd +
CentralAir, data = train_test)
grid = 10^seq(1, -5, length = 100)
#Running 5-fold cross validation.
set.seed(0)
cv.ridge = cv.glmnet(x, y, lambda = grid, alpha = 0, nfolds = 10)
plot(cv.ridge, main = "Ridge Regression\n")
best.lambda.ridge = cv.ridge$lambda.min
best.lambda.ridge # 0.01
#What is the test MSE associated with this best value of lambda?
ridge.models.train = glmnet(x, y, alpha = 0, lambda = grid)
ridge.best.lambda.train = predict(ridge.models.train, s = best.lambda.ridge, newx = x.test)
sqrt(mean((ridge.best.lambda.train - y.test)^2)) #0.04010766
mean(abs(ridge.best.lambda.train - y.test)) #0.0968727 , absolute error
#or
ridge.best.lambda.train = predict.cv.glmnet(cv.ridge, s ="lambda.min", newx = x.test)
mean((ridge.best.lambda.train - y.test)^2) #0.04010766
lasso_model = glmnet(x, y, alpha = 1, lambda = grid)
plot(lasso_model, xvar = "lambda", label = TRUE, main = "Lasso Regression")
#Running 5-fold cross validation.
set.seed(0)
cv.lasso = cv.glmnet(x, y,
lambda = grid, alpha = 1, nfolds = 10)
plot(cv.lasso, main = "Lasso Regression\n")
best.lambda.lasso = cv.lasso$lambda.min
best.lambda.lasso #0.01
# What is the test MSE associated with this best value of lambda?
lasso.model.train = glmnet(x, y, alpha = 1, lambda = grid)
lasso.best.lambda.train = predict(lasso.model.train, s = best.lambda.lasso, newx = x.test)
summary(lasso.best.lambda.train)
plot(lasso.best.lambda.train)
mean((lasso.best.lambda.train - y.test)^2) # 0.03932375
#or
lasso.best.lambda.train = predict.cv.glmnet(cv.lasso, s ="lambda.min", newx = x.test)
mean((lasso.best.lambda.train - y.test)^2) # 0.03932375
lasso.model.train = glmnet(x, y, alpha = 1, lambda = 10)
mean(abs(y.test-predict(lasso.model.train,newx = x.test)))
y_predict=predict(lasso.model.train, newx = x.test)
y_predict
y.test
mean((y.test-y_predict)^2)
mean(abs(y.test-y_predict))
a.test1=model.matrix(data=a.test)
plot(lasso_model, xvar = "lambda", label = TRUE, main = "Lasso Regression")
