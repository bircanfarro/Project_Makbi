---
title: "R Notebook"
output: html_notebook
---

Most missing data (NA) columns were valid categories. Replace with 'None'? 

```{r}
setwd("~/DataScience/MachineLearningProject/data")
library(dplyr)
df = read.csv('train.csv')

# when LotFrontage=NA, 226 are detached family homes
#df1 = df  %>% filter(is.na(LotFrontage) & (BldgType=='1Fam'))
train = df


```

We need to do more cleaning and imputation: 
```{r}
library(VIM) #For the visualization and imputation of missing values.

VIM::aggr(train) #A graphical interpretation of the missing values and their
            #combinations within the dataset.
```

```{r}
# reproduce proportion of missing
myNa = is.na(train)
pNA = apply(myNa, 2, sum)/nrow(myNa) # sum along cols
barplot(pNA)
pNA[order(pNA,decreasing=TRUE)]

```

We know a lot of the NA values occur in columns in which they are a legitimate level. 
In those cases, replace the NA with None, since it is not really NA. 
```{r}
library(dplyr)
df = read.csv('train.csv', stringsAsFactors = FALSE)
train = df
train$PoolQC[is.na(train$PoolQC)]             = 'None'
train$PoolArea[is.na(train$PoolQC)]           = 0
train$MiscFeature[is.na(train$MiscFeature)]   = 'None'
train$Alley[is.na(train$Alley)]               = 'None'
train$Fence[is.na(train$Fence)]               = 'None'
train$FireplaceQu[is.na(train$FireplaceQu)]   = 'None'
train$GarageType[is.na(train$GarageType)]     = 'None'
train$GarageFinish[is.na(train$GarageFinish)] = 'None'
train$GarageQual[is.na(train$GarageQual)]     = 'None'
train$GarageCond[is.na(train$GarageCond)]     = 'None'
train$BsmtCond[is.na(train$BsmtCond)]         = 'None'
train$BsmtQual[is.na(train$BsmtQual)]         = 'None'
train$BsmtExposure[is.na(train$BsmtExposure)] = 'None'
train$BsmtFinType1[is.na(train$BsmtFinType1)] = 'None'
train$BsmtFinType2[is.na(train$BsmtFinType2)] = 'None'
train$MasVnrType[is.na(train$MasVnrType)] = 'None' #assume missing means no veneer
train$MasVnrArea[is.na(train$MasVnrArea)] = 0 # none in Veneer Type is always 0 in Veneer Area
train$Electrical[is.na(train$Electrical)] = 'SBrkr' # missing is imputed as the majority class (majority by large margin)


write.csv(train, file='train1.csv', row.names = FALSE)


rm('df')
rm('train')
df = read.csv('train1.csv')
train = df


# reproduce proportion of missing
myNa = is.na(train)
pNA = apply(myNa, 2, sum)/nrow(myNa) # sum along cols
barplot(pNA)
pNA[order(pNA,decreasing=TRUE)]


# Impute missing LotFrontage = median LotFrontage of same LotConfig, Neighborhood. 
# This could still leave NA if there is no more data of the same LotConfig, Neighborhood. 
# We can do a second pass where we impute the still-missing NA = median of LotFrontage of same LotConfig

lconfig_neigh_median_lookup = train %>% 
  group_by(LotConfig, Neighborhood) %>% 
  summarise(x=median(LotFrontage,na.rm=TRUE))
train = left_join(train, lconfig_neigh_median_lookup, by=c('LotConfig', 'Neighborhood'))
train[is.na(train$LotFrontage), "LotFrontage"] = train[is.na(train$LotFrontage),"x"]
train$x = NULL # drop the helper column "x"

lconfig_median_lookup = train %>%
  group_by(LotConfig)         %>%
  summarise(x=median(LotFrontage, na.rm=TRUE))
train = left_join(train, lconfig_median_lookup, by=c('LotConfig'))
train[is.na(train$LotFrontage), "LotFrontage"] = train[is.na(train$LotFrontage),"x"]
train$x = NULL

write.csv(train, file='train_clean.csv', row.names = FALSE)

# drop the only numerical colum in which we have legitimate NAs (when there is no garage).
# I am dropping it to provide a totally-free of NAs dataset, for algorithms that choke on NA. 
train$GarageYrBlt = NULL  
write.csv(train, file='train_superclean.csv', row.names = FALSE)

```

Lot Frontage is missing values for all categories of LotConfiguation, i.e. "Inside" lots have Lot Frontage, as do "CuldSac" lots. I also see that Inside lots in Edwards neighborhood have median = 60, whereas CulDSac lots in Edwards have 35. From this (and the below), it is clear that lot frontage is determined by LotConfig and Neighborhood, that is why we impute it where missing as the median of rows of same LotConfig and Neighborhood. 
```{r}
train[train$LotConfig=='Inside', c("LotFrontage","Neighborhood")] %>% group_by(Neighborhood) %>% summarise(n=median(LotFrontage, na.rm=TRUE))
```

```{r}
train[train$LotConfig=='CulDSac', c("LotFrontage","Neighborhood")] %>% group_by(Neighborhood) %>% summarise(n=median(LotFrontage, na.rm=TRUE))
```


Following is code to compute the correlation of all numerical columns. 

```{r}
library(corrplot)
# correlation matrix
# keep numerical columns
numericVars <- which(sapply(train, is.numeric)) #index vector numeric variables
numericVarNames <- names(numericVars) #saving names vector for use later

# correlation matrix
train_numVar <- train[, numericVars]
cor_numVar <- cor(train_numVar, use="pairwise.complete.obs") #correlations of all numeric variables

#sort on decreasing correlations with SalePrice
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))

#select only high correlations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.1)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]

corrplot(cor_numVar, tl.col="black", tl.pos = "lt")
corrplot(cor_numVar, method = "ellipse")
corrplot(cor_numVar, method = "color")


```

Let's now try to order our variables in the order of the t-stats of univariate regressions. 
The problem with this approach is that we'd have not defence to multi-collinearity. 
```{r}


# replace YearBuilt, YearRemodAdd, GarageYrBlt by AgeBuilt, AgeRemodAdd, AgeGarageBlt respectively
train$AgeBuilt     = 2018 - train$YearBuilt
train$AgeRemodAdd  = 2018 - train$YearRemodAdd
train$AgeGarageBlt = 2018 - train$GarageYrBlt
train$YearBuilt    = NULL
train$YearRemodAdd = NULL
train$GarageYrBlt  = NULL

numericVars = which(sapply(train, is.numeric)) #index vector numeric variables
numericVarNames = names(numericVars) #saving names vector for use later

# Collinearities and High Correlations
# 1. GrLivArea = X1stFlrSF + X2ndFlrSF
# 2. BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF = TotalBsmtSF
# 3. cor(TotalBsmtSF, X1stFlrSF) = 0.82, so eliminate one?
# 4. cor(GarageArea, GarageCars) = 0.88, so eliminatte one? Bircan did two regressions and found GarageCars is marginally better. 

# I eliminate some categorical numerical columns (categories coded as numbers)
# manually here. But I need to write some smarter code to eliminate them 
# automatically. 
numericVarNames = numericVarNames[!numericVarNames %in% 
                c("Id", "SalePrice", "OverallQual", 
                  "GarageArea", # "GarageCars", # we think GarageCars is more significant than GarageArea
                  # "YearBuilt", "YearRemodAdd", "GarageYrBlt", # keep these as numerical vars influencing the price
                  # "Fireplaces", "FullBath",  # keep counts: if you add one more bathroom, you can make that much more in sale price
                  "MSSubClass",   # categorical
                  # "HalfBath", "BsmtFullBath", "BsmtHalfBath", "KitchenAbvGr", "BedroomAbvGr",  # keep counts
                  # "EnclosedPorch",  # square footage variable, include it
                  # "ScreenPorch", # screen porch area in sq feet, include it
                  "OverallCond",
                  "GrLivArea", # eliminated because collinearity GrLivArea = X1stFlrSF + X2ndFlrSF
                  "TotalBsmtSF", # eliminated because collinearity BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF = TotalBsmtSF
                  # "TotRmsAbvGrd", # count of rooms, include 
                  "MoSold", "YrSold", "MiscVal")] 

#MiscVal and MiscFeature: value of misc feature, but this is odd: "elevator" etc. Hard to predict, eliminate it ???

nVars = length(numericVarNames)
PValExtract = function(ireg)
{
  varname = numericVarNames[ireg]
  model = lm(as.formula(paste('SalePrice ~ ', varname)), data=train)  
  coeff = coef(summary(model))
  pval = coeff[2,"Pr(>|t|)"]  
}

pvals = sapply(1:nVars, PValExtract)
bestPredictors = data.frame(name=numericVarNames,pval=pvals)
bestPredictors = bestPredictors[order(bestPredictors$pval),]
# got 16 best predictors
print(bestPredictors)
```

So the 27 predictors based on order of univariate t-vals are
```{r}
print(bestPredictors$name)
```

Let's now see if we get the same set of predictors following Bircan's method + eliminating the names above. 
```{r}
library(MASS) 
library(car)
numericVars = which(sapply(train, is.numeric)) #index vector numeric variables
numericVarNames = names(numericVars) #saving names vector for use later


# I eliminate some categorical numerical columns (categories coded as numbers)
# manually here. But I need to write some smarter code to eliminate them 
# automatically. 
numericVarNames = numericVarNames[!numericVarNames %in% 
                c("Id", "SalePrice", "OverallQual", 
                  "GarageArea", # "GarageCars", # we think GarageCars is more significant than GarageArea
                  # "YearBuilt", "YearRemodAdd", "GarageYrBlt", # keep these as numerical vars influencing the price
                  # "Fireplaces", "FullBath",  # keep counts: if you add one more bathroom, you can make that much more in sale price
                  "MSSubClass",   # categorical
                  # "HalfBath", "BsmtFullBath", "BsmtHalfBath", "KitchenAbvGr", "BedroomAbvGr",  # keep counts
                  # "EnclosedPorch",  # square footage variable, include it
                  # "ScreenPorch", # screen porch area in sq feet, include it
                  "OverallCond",
                  "GrLivArea", # eliminated because collinearity GrLivArea = X1stFlrSF + X2ndFlrSF
                  "TotalBsmtSF", # eliminated because collinearity BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF = TotalBsmtSF
                  # "TotRmsAbvGrd", # count of rooms, include 
                  "MoSold", "YrSold", "MiscVal")] 

train_numVar = na.omit(train[, c(numericVarNames, "SalePrice")])

model.empty = lm(SalePrice ~ 1, data = na.omit(train_numVar)) #The model with an intercept ONLY.
model.full = lm(SalePrice ~ ., data = na.omit(train_numVar)) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))

#Stepwise regression using AIC as the criteria (the penalty k = 2).
forwardAIC = step(model.empty, scope, direction = "forward", k = 2, trace=FALSE) #k is degrees of freedom, k=2 makes it AIC, according to doc

#Checking the model summary
#Found 11 variables with forward AIC. 
listcoef = coef(forwardAIC)
bestAICPredictorNames = names(listcoef)[-1] #11 predictors by AIC (Bircan approach)
bestTVALPredictorNames = bestPredictors$name #16 predictors by order of univariate t-values (Marius approach)

# Intersection of AIC predictors and univariate t-val predictors
commonPredictors = intersect(bestAICPredictorNames, bestTVALPredictorNames) # still 11, so the AIC predictors are a subset of those found by univariate tval
print(commonPredictors)
```

Best TVAL AIC predictors: 
```{r}
print(bestTVALPredictorNames)
```

Best forward AIC predictors:
```{r}
print(length(bestAICPredictorNames))
print(bestAICPredictorNames)
```

Does TVAL include AIC variables:
```{r}
print(setdiff(bestTVALPredictorNames, bestAICPredictorNames ))
print(setdiff(bestAICPredictorNames, bestTVALPredictorNames)) # zero, so TVAL includes AIC
```

Why are some variables in my approach eliminated in the AIC approach ? Because they introduced collinearities
```{r}
notAICPredictors = setdiff(bestTVALPredictorNames, bestAICPredictorNames)
print(notAICPredictors)
```


I gave my forward AIC set to Bircan, who found that I have 5 extra variables that she does not have. She then added each variable incrementally to her best model, and found that for some of the variables, she gets better AIC values than what she saw before. 

Bircan pushed her code, so the best model is one arising from her best model + some of my variables. Clean up a bit and decide ?
