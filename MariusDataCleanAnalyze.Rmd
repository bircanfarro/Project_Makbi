---
title: "R Notebook"
output: html_notebook
---

Most missing data (NA) columns were valid categories. Replace with 'None'? 

```{r}
setwd("~/DataScience/MachineLearningProject/data")
library(dplyr)
df = read.csv('train.csv')

# when LotFrontage=NA, 226 are detached family homes
#df1 = df  %>% filter(is.na(LotFrontage) & (BldgType=='1Fam'))
train = df


```

Following is code to compute the correlation of all numerical columns. 

```{r}
library(corrplot)
# correlation matrix
# keep numerical columns
numericVars <- which(sapply(train, is.numeric)) #index vector numeric variables
numericVarNames <- names(numericVars) #saving names vector for use later

# correlation matrix
train_numVar <- train[, numericVars]
cor_numVar <- cor(train_numVar, use="pairwise.complete.obs") #correlations of all numeric variables

#sort on decreasing correlations with SalePrice
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))

#select only high correlations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.1)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]

corrplot(cor_numVar, tl.col="black", tl.pos = "lt")
corrplot(cor_numVar, method = "ellipse")
corrplot(cor_numVar, method = "color")


```

Let's now try to order our variables in the order of the t-stats of univariate regressions. 
The problem with this approach is that we'd have not defence to multi-collinearity. 
```{r}


# replace YearBuilt, YearRemodAdd, GarageYrBlt by AgeBuilt, AgeRemodAdd, AgeGarageBlt respectively
train$AgeBuilt     = 2018 - train$YearBuilt
train$AgeRemodAdd  = 2018 - train$YearRemodAdd
train$AgeGarageBlt = 2018 - train$GarageYrBlt
train$YearBuilt    = NULL
train$YearRemodAdd = NULL
train$GarageYrBlt  = NULL

numericVars = which(sapply(train, is.numeric)) #index vector numeric variables
numericVarNames = names(numericVars) #saving names vector for use later

# Collinearities and High Correlations
# 1. GrLivArea = X1stFlrSF + X2ndFlrSF
# 2. BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF = TotalBsmtSF
# 3. cor(TotalBsmtSF, X1stFlrSF) = 0.82, so eliminate one?
# 4. cor(GarageArea, GarageCars) = 0.88, so eliminatte one? Bircan did two regressions and found GarageCars is marginally better. 

# I eliminate some categorical numerical columns (categories coded as numbers)
# manually here. But I need to write some smarter code to eliminate them 
# automatically. 
numericVarNames = numericVarNames[!numericVarNames %in% 
                c("Id", "SalePrice", "OverallQual", 
                  "GarageArea", # "GarageCars", # we think GarageCars is more significant than GarageArea
                  # "YearBuilt", "YearRemodAdd", "GarageYrBlt", # keep these as numerical vars influencing the price
                  # "Fireplaces", "FullBath",  # keep counts: if you add one more bathroom, you can make that much more in sale price
                  "MSSubClass",   # categorical
                  # "HalfBath", "BsmtFullBath", "BsmtHalfBath", "KitchenAbvGr", "BedroomAbvGr",  # keep counts
                  # "EnclosedPorch",  # square footage variable, include it
                  # "ScreenPorch", # screen porch area in sq feet, include it
                  "OverallCond",
                  "GrLivArea", # eliminated because collinearity GrLivArea = X1stFlrSF + X2ndFlrSF
                  "TotalBsmtSF", # eliminated because collinearity BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF = TotalBsmtSF
                  # "TotRmsAbvGrd", # count of rooms, include 
                  "MoSold", "YrSold", "MiscVal")] 

#MiscVal and MiscFeature: value of misc feature, but this is odd: "elevator" etc. Hard to predict, eliminate it ???

nVars = length(numericVarNames)
PValExtract = function(ireg)
{
  varname = numericVarNames[ireg]
  model = lm(as.formula(paste('SalePrice ~ ', varname)), data=train)  
  coeff = coef(summary(model))
  pval = coeff[2,"Pr(>|t|)"]  
}

pvals = sapply(1:nVars, PValExtract)
bestPredictors = data.frame(name=numericVarNames,pval=pvals)
bestPredictors = bestPredictors[order(bestPredictors$pval),]
# got 16 best predictors
print(bestPredictors)
```

So the 27 predictors based on order of univariate t-vals are
```{r}
print(bestPredictors$name)
```

Let's now see if we get the same set of predictors following Bircan's method + eliminating the names above. 
```{r}
library(MASS) 
library(car)
numericVars = which(sapply(train, is.numeric)) #index vector numeric variables
numericVarNames = names(numericVars) #saving names vector for use later


# I eliminate some categorical numerical columns (categories coded as numbers)
# manually here. But I need to write some smarter code to eliminate them 
# automatically. 
numericVarNames = numericVarNames[!numericVarNames %in% 
                c("Id", "SalePrice", "OverallQual", 
                  "GarageArea", # "GarageCars", # we think GarageCars is more significant than GarageArea
                  # "YearBuilt", "YearRemodAdd", "GarageYrBlt", # keep these as numerical vars influencing the price
                  # "Fireplaces", "FullBath",  # keep counts: if you add one more bathroom, you can make that much more in sale price
                  "MSSubClass",   # categorical
                  # "HalfBath", "BsmtFullBath", "BsmtHalfBath", "KitchenAbvGr", "BedroomAbvGr",  # keep counts
                  # "EnclosedPorch",  # square footage variable, include it
                  # "ScreenPorch", # screen porch area in sq feet, include it
                  "OverallCond",
                  "GrLivArea", # eliminated because collinearity GrLivArea = X1stFlrSF + X2ndFlrSF
                  "TotalBsmtSF", # eliminated because collinearity BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF = TotalBsmtSF
                  # "TotRmsAbvGrd", # count of rooms, include 
                  "MoSold", "YrSold", "MiscVal")] 

train_numVar = na.omit(train[, c(numericVarNames, "SalePrice")])

model.empty = lm(SalePrice ~ 1, data = na.omit(train_numVar)) #The model with an intercept ONLY.
model.full = lm(SalePrice ~ ., data = na.omit(train_numVar)) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))

#Stepwise regression using AIC as the criteria (the penalty k = 2).
forwardAIC = step(model.empty, scope, direction = "forward", k = 2, trace=FALSE) #k is degrees of freedom, k=2 makes it AIC, according to doc

#Checking the model summary
#Found 11 variables with forward AIC. 
listcoef = coef(forwardAIC)
bestAICPredictorNames = names(listcoef)[-1] #11 predictors by AIC (Bircan approach)
bestTVALPredictorNames = bestPredictors$name #16 predictors by order of univariate t-values (Marius approach)

# Intersection of AIC predictors and univariate t-val predictors
commonPredictors = intersect(bestAICPredictorNames, bestTVALPredictorNames) # still 11, so the AIC predictors are a subset of those found by univariate tval
print(commonPredictors)
```

Best TVAL AIC predictors: 
```{r}
print(bestTVALPredictorNames)
```

Best forward AIC predictors:
```{r}
print(length(bestAICPredictorNames))
print(bestAICPredictorNames)
```

Does TVAL include AIC variables:
```{r}
print(setdiff(bestTVALPredictorNames, bestAICPredictorNames ))
print(setdiff(bestAICPredictorNames, bestTVALPredictorNames)) # zero, so TVAL includes AIC
```

Why are some variables in my approach eliminated in the AIC approach ? Because they introduced collinearities
```{r}
notAICPredictors = setdiff(bestTVALPredictorNames, bestAICPredictorNames)
print(notAICPredictors)
```


I gave my forward AIC set to Bircan, who found that I have 5 extra variables that she does not have. She then added each variable incrementally to her best model, and found that for some of the variables, she gets better AIC values than what she saw before. 

Bircan pushed her code, so the best model is one arising from her best model + some of my variables. Clean up a bit and decide ?
